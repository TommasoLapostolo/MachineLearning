{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: SUPPORT VECTOR MACHINES\n",
    "\n",
    "Course 2024/25: *M. Caligiuri*, *M. Pavan*, *P. Zanuttigh*\n",
    "\n",
    "The notebook contains some simple tasks to be performed with **SUPPORT VECTOR MACHINES (SVM)**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Classification with Support Vector Machines\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVM) for weather classification. We will use a dataset collected using the Luxottica **iSee** glasses. These devices provide multiple **sensors mounted inside the glasses**, which can be accessed through a bluetooth connection.\n",
    "\n",
    "![I-SEE Glasses](data/isee.png \"I-SEE\")\n",
    "\n",
    "The dataset corresponds to 8 hours of atmospherical data recordings sampled every 3 seconds.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| ID  | Label       |\n",
    "| :-: | :-:         |\n",
    "| 0   | Sunny       |\n",
    "| 1   | Rain        |\n",
    "| 2   | Cloudy      |\n",
    "| 3   | Mostly Clear|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary step\n",
    "\n",
    "Place your **name** and **ID number** (matricola) in the cell below. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Tommaso Tombolato\n",
    "\n",
    "**ID Number**: 2104231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the helper functions\n",
    "\n",
    "In this section you will find some helper functions (some already implemented, some to be implemented by you) that will be used in the following sections.\n",
    "1. `load_dataset` -> to load the dataset from the file `data/lux.npz`,\n",
    "2. `plot_input` -> to plot the input data,\n",
    "3. `k_split` ->  to split the training dataset in k different folds,\n",
    "4. `k_fold_cross_validation` -> to perform the k-fold cross validation.\n",
    "\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the dataset\n",
    "def load_dataset(path: str) -> tp.Tuple[np.ndarray, np.ndarray]:\n",
    "    with np.load(path) as data:\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        \n",
    "        # Normalize the data\n",
    "        x -= x.mean(axis=0)\n",
    "        x /= x.std(axis=0)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix: np.ndarray, labels: np.ndarray) -> None:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    cmap = plt.cm.get_cmap('Accent', 4)\n",
    "    im = ax.scatter(X_matrix[:,0], X_matrix[:,1], X_matrix[:,2], c=labels, cmap=cmap)\n",
    "    im.set_clim(-0.5, 3.5)\n",
    "    cbar=fig.colorbar(im, ticks=[0,1,2,3], orientation='vertical', cmap=cmap)\n",
    "    cbar.ax.set_yticklabels(['Sunny', 'Rainy','Cloudy', 'Mostly clear']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset in k different folds\n",
    "def k_split(x: np.ndarray, y:np.ndarray, k: int, shuffle: bool = True) -> tp.Tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    # Shuffle the dataset\n",
    "    if shuffle:\n",
    "        # Create a list of indices\n",
    "        idx = np.arange(x.shape[0])\n",
    "        # Randomly shuffle the indices\n",
    "        np.random.shuffle(idx)\n",
    "        # Shuffle the dataset\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    # Split the dataset in k folds\n",
    "    a = np.split(x, k)\n",
    "    b = np.split(y, k)\n",
    "\n",
    "    return a, b # mi ritorna una tupla;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the k-fold cross validation\n",
    "def k_fold_cross_validation(x_train: np.ndarray, y_train: np.ndarray, k: int, model: SVC, parameters: dict) -> tp.Tuple[tuple, tuple]:\n",
    "    # Define the folds for the cross validation\n",
    "    x_folds, y_folds = k_split(x_train, y_train, k)\n",
    "\n",
    "    # Build a list containing all of the possible combinatioon of parameters\n",
    "    params = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    # Initialize the dictionary of results\n",
    "    results = {k: 0 for k in params}\n",
    "\n",
    "    # For each param combination, perform the SVM training and testing\n",
    "    for param in params:\n",
    "        param = dict(zip(parameters.keys(), param))\n",
    "\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        # ciclo sui k folds\n",
    "        for i in range(k):\n",
    "            # validation fold\n",
    "            x_val, y_val = x_folds[i], y_folds[i]  \n",
    "            # training folds = tutti quelli diversi da i\n",
    "            x_tr = np.vstack([x_folds[j] for j in range(k) if j != i]) # vstack concatenazione verticale\n",
    "            y_tr = np.hstack([y_folds[j] for j in range(k) if j != i]) # hstack concatenazione orrizzonatale\n",
    "            \n",
    "            # istanzio un nuovo SVC con quei parametri\n",
    "            clf = SVC(**param)\n",
    "            clf.fit(x_tr, y_tr)\n",
    "\n",
    "             # accuracy sul fold di validazione\n",
    "            fold_accuracies.append(clf.score(x_val, y_val))\n",
    "            \n",
    "        # Compute the mean accuracy\n",
    "        results[tuple(param.values())] = round(np.mean(fold_accuracies), 4)\n",
    "    \n",
    "    # Find the best parameters\n",
    "    best_parameters = dict(zip(parameters.keys(), params[np.argmax(list(results.values()))]))\n",
    "    best_accuracy = np.max(list(results.values()))\n",
    "    best = (best_parameters, best_accuracy)\n",
    "\n",
    "    # Add the param name to the results\n",
    "    results = [({k: v for k, v in zip(parameters.keys(), p)}, a) for p, a in results.items()]\n",
    "\n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Hyper-parameters search\n",
    "\n",
    "### TO DO (A.0)\n",
    "\n",
    "**Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: 1234567 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "# as usual you can try different seeds by adding a constant to the number:\n",
    "# ID = 1234567 + X\n",
    "ID = 2104231 # YOUR ID (replace None with your ID)\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset and split it** in training and test set (while the **training** set is **typically larger**, here we set the number of training samples to 1000 and 4000 for the test data).\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the helper function\n",
    "X, y = load_dataset(\"data/lux.npz\")\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task is quite easy, let's add noise to make it more challenging!\n",
    "# You can try without noise (comment the next 2 lines, easy task), with the suggested amount of noise,\n",
    "# or play with the suggested amount of noise \n",
    "\n",
    "noise = np.random.normal(0, 0.1, X.shape)\n",
    "X = X + noise\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.1)\n",
    "\n",
    "**Divide** the **data into training and test set** (for this part use 1000 samples in the **first** set, 4000 in the **second** one). Make sure that each label is present at least 10 times in training. If it is not, then keep adding permutations to the initial data until this happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permute the data and split into training and test taking the first 1000\n",
    "# data samples as training and 4000 samples as test\n",
    "permutation = True\n",
    "\n",
    "while permutation == True:\n",
    "    # Create a list of indices\n",
    "    idx = np.arange(X.shape[0])\n",
    "    # Randomly shuffle the indices\n",
    "    np.random.shuffle(idx)\n",
    "    # Shuffle the dataset\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    a, b, c, d = 0, 0, 0, 0\n",
    "    for i in range(1000):\n",
    "        if y[i] == 0:\n",
    "            a = a + 1\n",
    "        elif y[i] == 1: # in Python non si scrive else if, ma elif.\n",
    "            b = b + 1\n",
    "        elif y[i] == 2:\n",
    "            c = c + 1\n",
    "        else:\n",
    "            d = d + 1\n",
    "\n",
    "    if (a > 9) and (b > 9) and (c > 9) and (d > 9):\n",
    "        permutation = False\n",
    "\n",
    "m_training = 1000\n",
    "m_test = 4000\n",
    "\n",
    "X_train = X[0:m_training, : ]\n",
    "X_test = X[m_training : m_training+m_test, : ]\n",
    "y_train = y[0 : m_training]\n",
    "y_test = y[m_training : m_training+m_test]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape,\"X_test shape:\", X_test.shape,\"||\",\"y_train shape:\",  y_train.shape,\"y_test shape:\", y_test.shape)\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True) # np.unique(y_train, return_counts=True) restituisce due array: labels: le etichette uniche presenti in y_train. freqs: il numero di volte che ogni etichetta appare in y_train.\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the plotting function\n",
    "plot_input(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.2)\n",
    "\n",
    "Use a SVM classfier with cross validation to pick a model. Use a 4-fold cross-validation. Let's start with a Linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [ 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Define the model (without parameters)\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, svm, parameters)\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "print(best[0]);\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "print(best[1])\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "print(results);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.3)\n",
    "\n",
    "Pick a model for the Polynomial kernel with degree=2. Note that the model for the polynomial kernel is slightly different from the one on the book, for the meaning of the \"gamma\" parameter see https://scikit-learn.org/stable/modules/svm.html#svm-kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "# Define an SVM with poly of degree 2 kernel (without parameters)\n",
    "poly2_svm = SVC(kernel='poly', degree=2)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, poly2_svm, parameters)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=2 KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "print(best[0]);\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "print(best[1])\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "print(results);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.4)\n",
    "\n",
    "Now let's try a higher degree for the polynomial kernel (e.g., 3rd degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for poly with higher degree kernel\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01,0.1, 1]}\n",
    "\n",
    "# Define an SVM with poly of higher degree kernel (without parameters)\n",
    "degree = 3\n",
    "poly_svm = SVC(kernel='poly', degree=degree)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, poly_svm, parameters)\n",
    "\n",
    "print (f\"RESULTS FOR POLY DEGREE={degree} KERNEL\")\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "print(best[0]);\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "print(best[1])\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "print(results);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.5)\n",
    "\n",
    "Pick a model for the Radial Basis Function kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for rbf SVM\n",
    "parameters = {'C': [0.1, 1, 10, 100],'gamma':[0.001, 0.01, 0.1,1]}\n",
    "\n",
    "# Define an SVM with rbf kernel (without parameters)\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, rbf_svm, parameters)\n",
    "\n",
    "print ('RESULTS FOR rbf KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "print(best[0]);\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "print(best[1])\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "print(results);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q1) [Answer the following]\n",
    "\n",
    "What do you observe when using RBF and polynomial kernels on this dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q1:**: Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.6)\n",
    "Report here the best SVM kernel and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "best_svm = SVC(kernel='rbf', C =10, gamma = 1)\n",
    "\n",
    "# Run the svm model on the whole training set\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Compute the errors\n",
    "# (error is 1 - svm.score)\n",
    "training_error = 1 - best_svm.score(X_train, y_train)\n",
    "test_error = 1 - best_svm.score(X_test, y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.7)\n",
    "\n",
    "Analyze how the gamma parameter (inversely proportional to standard deviation of Gaussian Kernel) impact the performances of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different values of gamma\n",
    "# use rbf kernel and C=1\n",
    "\n",
    "# Set gamma values\n",
    "gamma_values = np.logspace(-5,2,8)\n",
    "print(gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list, test_acc_list = [], []\n",
    "\n",
    "# ADD YOUR CODE TO TRAIN THE SVM MULTIPLE TIMES WITH THE DIFFERENT VALUES OF GAMMA\n",
    "# PLACE THE TRAIN AND TEST ACCURACY FOR EACH TEST IN THE TRAIN AND TEST ACCURACY LISTS\n",
    "# al variare di gamma\n",
    "for i in range(len(gamma_values)):\n",
    "    svm = SVC(kernel='rbf', C =1, gamma = gamma_values[i])\n",
    "    \n",
    "    # Run the svm model on the whole training set\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute the errors\n",
    "    # (error is 1 - svm.score)\n",
    "    train_acc_list.append(1 - svm.score(X_train, y_train))\n",
    "    test_acc_list.append(1 - svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(gamma_values, train_acc_list)\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('gamma')\n",
    "ax[0].set_ylabel('Train accuracy')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(gamma_values, test_acc_list)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('gamma')\n",
    "ax[1].set_ylabel('Test accuracy')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) More data\n",
    "Now let's do the same but using more data points for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.0)\n",
    "\n",
    "Choose a higher number of data points (e.g. x = 10000) for training data (select the number depending on your computing capability.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "\n",
    "permutation = np.random.permutation(len(y))\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "\n",
    "m_training = 10000 #int(len(y)*3/4) # adjust depending on the capabilities of your PC (replace None)\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)\n",
    "\n",
    "# initialize support variables for boundaries visualization\n",
    "granularity = 25\n",
    "x_max = np.abs(X).max()\n",
    "x_range = np.linspace(-x_max, x_max, granularity)\n",
    "x_grid = np.stack(np.meshgrid(x_range, x_range, x_range)).reshape(3, -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.1)\n",
    "\n",
    "Let's try to use SVM with parameters obtained from the best model for $m_{training} =  10000$. Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the TO DO (C.Q1) cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "\n",
    "# Get training and test error for the best SVM model from CV\n",
    "best_svm = SVC(kernel='rbf', C =10, gamma = 1)\n",
    "\n",
    "# Run the svm model on the whole training set\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Compute the errors\n",
    "# (error is 1 - svm.score)\n",
    "training_error = 1 - best_svm.score(X_train, y_train)\n",
    "test_error = 1 - best_svm.score(X_test, y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Boundaries Visualization\n",
    "\n",
    "Now let us plot the classification boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.0)\n",
    "\n",
    "Use the SVM to predict on the test set X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_test = (SVC(kernel='rbf').fit(X_train, y_train)).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constructed a grid of all possible combinations of input values, we now use it to extract the classification boundaries of the three classifiers by having them predict on each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_SVM_grid = rbf_svm.predict(x_grid)\n",
    "\n",
    "rbf_SVM_m = y_test == rbf_svm_test\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "ax1.scatter(x_grid[:,0], x_grid[:,1], x_grid[:,2], c=rbf_SVM_grid, linewidth=0, marker=\"s\", alpha=.05,cmap='Accent')\n",
    "\n",
    "ax1.scatter(X_test[rbf_SVM_m,0], X_test[rbf_SVM_m,1], X_test[rbf_SVM_m,2], c=y_test[rbf_SVM_m], linewidth=.5, edgecolor=\"k\", marker=\".\",cmap='Accent')\n",
    "ax1.scatter(X_test[~rbf_SVM_m,0], X_test[~rbf_SVM_m,1], X_test[~rbf_SVM_m,2], c=y_test[~rbf_SVM_m], linewidth=1, edgecolor=\"r\", marker=\".\",cmap='Accent')\n",
    "ax1.set_xlim([-x_max, x_max])\n",
    "ax1.set_ylim([-x_max, x_max])\n",
    "ax1.set_zlim([-x_max, x_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q1) [Answer the following]\n",
    "\n",
    "Compare and discuss the results from SVM with m=600 and with m=10000 (or whatever value you set) training data points. If you stopped the SVM, include such aspect in your comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q1:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.1)\n",
    "\n",
    "Plot the confusion matrix for the SVM classifier. The confusion matrix has one column for each predicted label and one row for each true label. \n",
    "It shows for each class in the corresponding row how many samples belonging to that class gets each possible output label. Notice that the diagonal contains the correctly classified samples, while the other cells correspond to errors. You can obtain it with the sklearn.metrics.confusion_matrix function (see the documentation). You can also print also the normalized confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) # for better aligned printing of confusion matrix use floatmode='fixed'\n",
    "\n",
    "u, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Labels and frequencies in test set: \", counts)\n",
    "\n",
    "confusion_SVM = skm.confusion_matrix(y_test, rbf_svm_test)\n",
    "print(\"\\n Confusion matrix SVM  \\n \\n\", confusion_SVM)\n",
    "print(\"\\n Confusion matrix SVM (normalized)   \\n \\n\", confusion_SVM /counts[:,None] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "    \n",
    "im = plt.imshow(confusion_SVM /counts[:,None], cmap=\"Blues\",interpolation='nearest')\n",
    "plt.xticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "plt.yticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "cm = confusion_SVM /counts[:,None]\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        ha=\"center\", va=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.colorbar(im, location='bottom')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q2) [Answer the following]\n",
    "\n",
    "Have a look at the confusion matrix and comment on the obtained accuracies. Why some classes have lower accuracies and others an higher one? Make some guesses on the possible causes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q2:** Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
